
machine(L1Cache, "MSI L1 Cache")
: Sequencer * sequencer,
  CacheMemory * cacheMemory, //ECE552: Unified L1 Instruction and Data Cache
  int cache_response_latency = 12,
  int issue_latency = 2,
  bool send_evictions = true
{

  // NETWORK BUFFERS

  //=============================================================
  // To the Network
  // ==============
  //ECE552: <requestFromCache>: The local cache coherence controller gets requests from the L1 cache (e.g., on a miss) that it will propagate to the network via virtual network 2.
  //ECE552: <responseFromCache>: The local cache coherence controller also propagates responses from its local cache to the network (e.g., a response to a forwarded GetS request from the directory)

  MessageBuffer requestFromCache, network="To", virtual_network="2", ordered="true", vnet_type="request";
  MessageBuffer responseFromCache, network="To", virtual_network="4", ordered="true", vnet_type="response";

  //=============================================================
  // From the Network
  // ================
  //ECE552:  <forwardToCache>: The local cache coherence controller receives forwarded requests from the directory.
  //ECE552:  <responseToCache>: The local cache coherence controller also receives replies from the Network for requests it issues via virtual network 2.

  MessageBuffer forwardToCache, network="From", virtual_network="3", ordered="true", vnet_type="forward";
  MessageBuffer responseToCache, network="From", virtual_network="4", ordered="true", vnet_type="response";

  //=============================================================
  // STATES
  // ======
  //
  // ECE552: AccessPermissions specified in RubySlicc_Exports.sm
  // ECE552: Valid Data:   Read_Only, Read_Write
  // ECE552: Invalid Data: Invalid, NotPresent, Busy
  //=============================================================

  state_declaration(State, desc="Cache states", default="L1Cache_State_I") { //ECE552: All cache blocks are initially in Invalid State

    //=====================
    //ECE552: Stable States
    //=====================

    // Base States required by the MSI protocol.
    // These are 'implementation independent'.
    I, AccessPermission:Invalid, desc="Not Present/Invalid";
    S, AccessPermission:Read_Only, desc="Shared. Data can be read but not written.";
    M, AccessPermission:Read_Write, desc="Modified. Data can be read or written.";
   
    //========================
    //ECE552: Transient States
    //========================
    IS_D, AccessPermission:Busy, desc="Waiting for ro data.";
    IM_AD, AccessPermission:Busy, desc="Waiting for rw data and acknowledgement.";
    IM_A, AccessPermission:Busy, desc="Waiting for rw acknowledgement.";
    // I think this exists because it is possible that someone else got M, but we haven't gotten invalidation yet.
    SM_AD, AccessPermission:Read_Only, desc="Waiting for upgrade acknowledgement and data.";
    SM_A, AccessPermission:Read_Only, desc="Waiting for upgrade acknowledgement.";
    MI_A, AccessPermission:Read_Only, desc="Waiting for eviction ack.";
    SI_A, AccessPermission:Read_Only, desc="Waiting for eviction ack, from S.";
    II_A, AccessPermission:Busy, desc="Waiting for ack to drop to I.";
  }

  // =============================================================================
  // EVENTS
  // =============================================================================
  enumeration(Event, desc="Cache events") {

    //=================================================================================================================
    //ECE552: L1 Events triggered from the local processor explicitly (i.e., memory request) or implicitly (replacement)
    //=================================================================================================================
    Load,       desc="Load request from processor";
    Ifetch,     desc="Ifetch request from processor";
    Store,      desc="Store request from processor";

    Replacement,  desc="Replace L1 cache block";

    //========================================================================
    //ECE552: Requests forwarded to the L1 cache from the directory controller 
    //========================================================================
    Fwd_GetS,   desc="GetS forwarded from the directory"; //ECE552: original request was for read access
    Fwd_GetM,   desc="GetM forwarded from the directory"; //ECE552: original request was for write access
    Inv,        desc="Invalidate request from the directory";

    WB_Ack,     desc="Ack from the directory for a writeback";
    
    Inv_Ack,	 desc="Invalidate Ack";
    Inv_Ack_all, desc="Last Invalidate Ack";

    Data_from_Dir_No_Acks,       desc="Data from Directory, 0 Acks";
    Data_from_Dir_Ack_Cnt,       desc="Data from Directory, along with Ack Count";
    Data_from_Dir_Ack_Cnt_Last,  desc="Data from Directory, along with Ack Count, arrive after Inv_Ack";
    Data_from_Owner,		 desc="Data from remote L1 (remote Owner)";

  }

  // =============================================================================
  // STRUCTURE DEFINITIONS
  // =============================================================================

  MessageBuffer mandatoryQueue, ordered="false";

  // CacheEntry
  structure(Entry, desc="...", interface="AbstractCacheEntry") {
    State CacheState,        desc="cache state";
    bool Dirty,              desc="Is the data dirty (different than memory)?";
    DataBlock DataBlk,       desc="Data in the block";
  }

  // TBE fields
  // ECE552: Transient Block Entry acts like an MSHR (i.e., Miss Handling Register) keeping track of transitions that require an intermediate transient state
  structure(TBE, desc="...") {
    State TBEState,               desc="Transient state";
    DataBlock DataBlk,       	  desc="data for the block, required for concurrent writebacks";
    int pendingAcks, default="0", desc="number of pending acks";
  }

  structure(TBETable, external="yes") {
    TBE lookup(Address);
    void allocate(Address);
    void deallocate(Address);
    bool isPresent(Address);
  }


  // =============================================================================
  // STRUCTURES
  // =============================================================================
  TBETable TBEs, template_hack="<L1Cache_TBE>";

  // =============================================================================
  // PROTOTYPES
  // =============================================================================
  void set_cache_entry(AbstractCacheEntry a);
  void unset_cache_entry();
  void set_tbe(TBE b);
  void unset_tbe();

  Entry getCacheEntry(Address address), return_by_pointer="yes" {
    return static_cast(Entry, "pointer", cacheMemory.lookup(address));
  }

  // =============================================================================
  // FUNCTIONS
  // =============================================================================

  //ECE552: This function maps a Ruby Incoming Memory Request Type to a SLICC Event
  Event mandatory_request_type_to_event(RubyRequestType type) {
   if (type == RubyRequestType:LD) {
      return Event:Load;
    } else if (type == RubyRequestType:IFETCH) {
      return Event:Ifetch;
    } else if ((type == RubyRequestType:ST) || (type == RubyRequestType:ATOMIC)) {
      return Event:Store;
    } else {
      error("Invalid RubyRequestType");
    }
  }

  //ECE552: This function returns the state for memory location <addr>. Note the priority:
  // We first check the TBE (in case there are pending requests), then the cache, 
  // and if nothing is present then we return an invalid state.

  State getState(TBE tbe, Entry cache_entry, Address addr) {

    if (is_valid(tbe)) {
      return tbe.TBEState;
    }
    else if (is_valid(cache_entry)) {
      return cache_entry.CacheState;
    }
    else {
      return State:I;
    }
  }

  void setState(TBE tbe, Entry cache_entry, Address addr, State state) {

    if (is_valid(tbe)) {
      tbe.TBEState := state;
    }

    if (is_valid(cache_entry)) {
      cache_entry.CacheState := state;
    }
  }

  // ECE552: Function L1Cache_State_to_permission() is autogenerated here: build/ALPHA/mem/protocol/L1Cache_State.cc
  // ECE552: The permissions are the ones set in the States part earlier in this file.
  // ECE552: Again note the order the different structures are checked: first the tbe and then the cache

  AccessPermission getAccessPermission(Address addr) {
    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      return L1Cache_State_to_permission(tbe.TBEState);
    }

    Entry cache_entry := getCacheEntry(addr);
    if(is_valid(cache_entry)) {
      return L1Cache_State_to_permission(cache_entry.CacheState);
    }

    return AccessPermission:NotPresent;
  }

  void setAccessPermission(Entry cache_entry, Address addr, State state) {
    if (is_valid(cache_entry)) {
      cache_entry.changePermission(L1Cache_State_to_permission(state));
    }
  }

  DataBlock getDataBlock(Address addr), return_by_ref="yes" {
    return getCacheEntry(addr).DataBlk;
  }

  int getPendingAcks(TBE tbe) {
    return tbe.pendingAcks;
  }

  GenericMachineType getNondirectHitMachType(MachineID sender) {
    if (machineIDToMachineType(sender) == MachineType:L1Cache) {
      //
      // NOTE direct local hits should not call this
      //
      return GenericMachineType:L1Cache_wCC; 
    } else {
      return ConvertMachToGenericMach(machineIDToMachineType(sender));
    }
  }


  // =============================================================================
  // NETWORK PORTS
  // =============================================================================


  //ECE552: Two output port that place messages onto the network (i.e., requests and reponses)
  out_port(requestNetwork_out, RequestMsg, requestFromCache);
  out_port(responseNetwork_out, ResponseMsg, responseFromCache);
 
  //ECE552: Process incoming request messages from the forwardToCache Network buffer. Trigger appropriate events based on the incoming CoherenceRequestType.
  //ECE552: All CoherenceRequestType declarations are provided in MSI-msg.sm

  in_port(forwardRequestNetwork_in, RequestMsg, forwardToCache) {
    if (forwardRequestNetwork_in.isReady()) {
      peek(forwardRequestNetwork_in, RequestMsg, block_on="Address") {

        Entry cache_entry := getCacheEntry(in_msg.Address);
        TBE tbe := TBEs[in_msg.Address];

        if (in_msg.Type == CoherenceRequestType:GETM) {
          DPRINTF(RubySlicc, "L1 - forwardRequestNetwork_in port: GETM request for address %s\n", in_msg.Address);
          trigger(Event:Fwd_GetM, in_msg.Address, cache_entry, tbe);
        }
	else if (in_msg.Type == CoherenceRequestType:GETS) {
          DPRINTF(RubySlicc, "L1 - forwardRequestNetwork_in port: GETS request for address %s\n", in_msg.Address);
          trigger(Event:Fwd_GetS, in_msg.Address, cache_entry, tbe);
        }
        else if (in_msg.Type == CoherenceRequestType:INV) {
          DPRINTF(RubySlicc, "L1 - forwardRequestNetwork_in port: INV request for address %s\n", in_msg.Address);
          trigger(Event:Inv, in_msg.Address, cache_entry, tbe);
        }
        else if (in_msg.Type == CoherenceRequestType:WB_ACK) {
          DPRINTF(RubySlicc, "L1 - forwardRequestNetwork_in port: WB_ACK messsage for address %s\n", in_msg.Address);
          trigger(Event:WB_Ack, in_msg.Address, cache_entry, tbe);
        }
        else {
          DPRINTF(RubySlicc, "Error! L1 - Unexpected Forwarded Message => address: %s, Request Type: %s\n", in_msg.Address, in_msg.Type);
          error("MSI_example L1 - Unexpected message in forwardToCache buffer");
        }
      }
    }
  }

  //ECE552: Process incoming response messages from the responseToCache Network buffer. Trigger appropriate events based on the incoming CoherenceResponseType.
  //ECE552: All CoherenceResponseType declarations are provided in MSI-msg.sm

  in_port(responseNetwork_in, ResponseMsg, responseToCache) {
    if (responseNetwork_in.isReady()) {
      peek(responseNetwork_in, ResponseMsg, block_on="Address") {

        Entry cache_entry := getCacheEntry(in_msg.Address);
        TBE tbe := TBEs[in_msg.Address];

        if (in_msg.Type == CoherenceResponseType:DATA_FROM_DIR) {
	  if (in_msg.AckCount == 0) {
            DPRINTF(RubySlicc, "L1 - responseNetwork_in port: DATA_FROM_DIR_NO_ACKS reply for address %s\n", in_msg.Address);
            trigger(Event:Data_from_Dir_No_Acks, in_msg.Address, cache_entry, tbe);
	  } else {
            if ( (getPendingAcks(tbe) - in_msg.AckCount) == 0 ) {
               DPRINTF(RubySlicc, "L1 - responseNetwork_in port: DATA_FROM_DIR_ACK_CNT reply for address %s - Data arrived after last Inv_Ack\n", in_msg.Address);
               trigger(Event:Data_from_Dir_Ack_Cnt_Last, in_msg.Address, cache_entry, tbe);
            } else {
               DPRINTF(RubySlicc, "L1 - responseNetwork_in port: DATA_FROM_DIR_ACK_CNT reply for address %s\n", in_msg.Address);
               trigger(Event:Data_from_Dir_Ack_Cnt, in_msg.Address, cache_entry, tbe);
            }
	  }
        }
        else if (in_msg.Type == CoherenceResponseType:DATA_FROM_OWNER) {
          DPRINTF(RubySlicc, "L1 - responseNetwork_in port: DATA_FROM_OWNER reply for address %s\n", in_msg.Address);
          trigger(Event:Data_from_Owner, in_msg.Address, cache_entry, tbe);
        } 
	else if (in_msg.Type == CoherenceResponseType:ACK) {
          if ( (getPendingAcks(tbe) - in_msg.AckCount) == 0 ) {
            DPRINTF(RubySlicc, "L1 - responseNetwork_in port: INV_Ack_all for address %s\n", in_msg.Address);
            trigger(Event:Inv_Ack_all, in_msg.Address, cache_entry, tbe);
          } else {
            DPRINTF(RubySlicc, "L1 - responseNetwork_in port: INV_Ack for address %s\n", in_msg.Address);
            trigger(Event:Inv_Ack, in_msg.Address, cache_entry, tbe);
          }
	}
        else {
          DPRINTF(RubySlicc, "Error! L1 - Unexpected Coherence Response Message => address: %s, Request Type: %s\n", in_msg.Address, in_msg.Type);
          error("MSI_example L1 - Unexpected message in reponseToCache buffer");
        }
      }
    }
  }

  //Mandatory Queue
  //ECE552: Process requests from the local processor.

  in_port(mandatoryQueue_in, RubyRequest, mandatoryQueue, desc="...") {
    if (mandatoryQueue_in.isReady()) {
      peek(mandatoryQueue_in, RubyRequest, block_on="LineAddress") {

        Entry cache_entry := getCacheEntry(in_msg.LineAddress);
        if (is_invalid(cache_entry) &&
            cacheMemory.cacheAvail(in_msg.LineAddress) == false ) {
          DPRINTF(RubySlicc, "L1 - mandatoryQueue_in request for address %s - Need to replace entry\n", in_msg.LineAddress);
          // make room for the block
          trigger(Event:Replacement, cacheMemory.cacheProbe(in_msg.LineAddress),
                  getCacheEntry(cacheMemory.cacheProbe(in_msg.LineAddress)),
                  TBEs[cacheMemory.cacheProbe(in_msg.LineAddress)]);
        }
        else {
          DPRINTF(RubySlicc, "L1 - mandatoryQueue_in request of type %s for address %s - No replacement necessary\n", in_msg.Type, in_msg.LineAddress);
          trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                  cache_entry, TBEs[in_msg.LineAddress]);
        }
      }
    }
  }

  // =============================================================================
  // ACTIONS
  // =============================================================================

  /* ECE552: A request is issued on a cache miss. Note that we should issue a different msg type
     based on original request (i.e., GETM for writes and GETS for reads).
     We need to specify the requestor core so we know where the data/ack counts etc will return to.
     We also need to specify the destination, assuming we have more than one directory controllers.
  */

  action(as_issueGetS, "as", desc="Issue a request for read-only access") {
    enqueue(requestNetwork_out, RequestMsg, latency=issue_latency) {
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:GETS;
      out_msg.Requestor := machineID;
      out_msg.Destination.add(map_Address_to_Directory(address));
      out_msg.MessageSize := MessageSizeType:Control;
    }
  }

  action(am_issueGetM, "am", desc="Issue a request for read-write access") {
    enqueue(requestNetwork_out, RequestMsg, latency=issue_latency) {
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:GETM;
      out_msg.Requestor := machineID;
      out_msg.Destination.add(map_Address_to_Directory(address));
      out_msg.MessageSize := MessageSizeType:Control;
    }
  }

  /* ECE552: Issue PUTS/PUTM requests, on local replacements to the directory */

  action(bs_issuePUTS, "bs", desc="Issue a PUTS request") {
    enqueue(requestNetwork_out, RequestMsg, latency=issue_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:PUTS;
      out_msg.Requestor := machineID;
      out_msg.Destination.add(map_Address_to_Directory(address));
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.MessageSize := MessageSizeType:Data;
    }
  }

  
  action(bm_issuePUTM, "bm", desc="Issue a PUTM request") {
    enqueue(requestNetwork_out, RequestMsg, latency=issue_latency) {
      assert(is_valid(cache_entry));
      out_msg.Address := address;
      out_msg.Type := CoherenceRequestType:PUTM;
      out_msg.Requestor := machineID;
      out_msg.Destination.add(map_Address_to_Directory(address));
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.MessageSize := MessageSizeType:Data;
    }
  }

  action(e_sendDataFromCacheToRequestor, "e", desc="Send data from cache to requestor") {
    peek(forwardRequestNetwork_in, RequestMsg) {
      enqueue(responseNetwork_out, ResponseMsg, latency=cache_response_latency) {
        assert(is_valid(cache_entry));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA_FROM_OWNER;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.DataBlk := cache_entry.DataBlk;
        out_msg.MessageSize := MessageSizeType:Response_Data;
      }
    }
  }

  action(ee_sendDataFromTBEToRequestor, "\e", desc="Send data from TBE to requestor") {
    peek(forwardRequestNetwork_in, RequestMsg) {
      enqueue(responseNetwork_out, ResponseMsg, latency=cache_response_latency) {
        assert(is_valid(tbe));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA_FROM_OWNER;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.DataBlk := tbe.DataBlk;
        out_msg.MessageSize := MessageSizeType:Response_Data;
      }
    }
  }

  action(de_sendDataFromCacheToDir, "de", desc="Send data from cache to directory") { 
     enqueue(responseNetwork_out, ResponseMsg, latency=cache_response_latency) { 
        assert(is_valid(cache_entry));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.Sender := machineID;
        out_msg.Destination.add(map_Address_to_Directory(address));
        out_msg.DataBlk := cache_entry.DataBlk;
        out_msg.MessageSize := MessageSizeType:Response_Data;
     }
  }

  action(dee_sendDataFromTBEToDir, "d\e", desc="Send data from TBE to directory") {
     enqueue(responseNetwork_out, ResponseMsg, latency=cache_response_latency) {
        assert(is_valid(tbe));
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:DATA;
        out_msg.Sender := machineID;
        out_msg.Destination.add(map_Address_to_Directory(address));
        out_msg.DataBlk := tbe.DataBlk;
	out_msg.MessageSize := MessageSizeType:Response_Data;
     }
  }

  action(fi_sendInvAck, "fi", desc="send Invalidation Acknowledgement to Requestor") {
    peek(forwardRequestNetwork_in, RequestMsg) {
      enqueue(responseNetwork_out, ResponseMsg, latency=cache_response_latency) {
        out_msg.Address := address;
        out_msg.Type := CoherenceResponseType:ACK;
        out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
        out_msg.MessageSize := MessageSizeType:Response_Control;
        out_msg.AckCount := 1;
      }
    }
  }


  action(i_allocateL1CacheBlock, "i", desc="Allocate a cache block") {
    if (is_valid(cache_entry)) {
      if (getAccessPermission(address) != AccessPermission:Invalid) {
	error("Should not get an allocate request for a block that has not Invalid permissions");
      }
    } else {
      set_cache_entry(cacheMemory.allocate(address, new Entry));
    }
  }

  action(h_deallocateL1CacheBlock, "h", desc="deallocate a cache block") {
    if (is_valid(cache_entry)) {
      cacheMemory.deallocate(address);
      unset_cache_entry();
    }
  }

  //ECE552: remove request issued by the local core from the mandatory incoming queue
  action(m_popMandatoryQueue, "m", desc="Pop the mandatory request queue") {
    mandatoryQueue_in.dequeue();
  }

  action(n_popResponseQueue, "n", desc="Pop the response queue") {
    profileMsgDelay(1, responseNetwork_in.dequeue_getDelayCycles());
  }

  action(o_popForwardedRequestQueue, "o", desc="Pop the forwarded request queue") {
    profileMsgDelay(2, forwardRequestNetwork_in.dequeue_getDelayCycles());
  }

  action(p_profileMiss, "p", desc="Profile cache miss") {
    peek(mandatoryQueue_in, RubyRequest) {
      cacheMemory.profileMiss(in_msg);
    }
  }

  /* ECE552: Load completed. Data block was present with Read permissions in local cache */
  action(r_load_hit, "r", desc="Notify sequencer the load completed.") {
    assert(is_valid(cache_entry));
    DPRINTF(RubySlicc,"%s\n", cache_entry.DataBlk);
    sequencer.readCallback(address, 
                           GenericMachineType:L1Cache,
                           cache_entry.DataBlk);
  }

  /* ECE552: Load completed. Block was not present in the local cache. 
     Got data either from the the directory or from another remote L1 cache */
  action(rx_load_hit, "rx", desc="External load completed.") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(is_valid(cache_entry));
      DPRINTF(RubySlicc,"%s\n", cache_entry.DataBlk);
      sequencer.readCallback(address, 
                             getNondirectHitMachType(in_msg.Sender),
                             cache_entry.DataBlk);
    }
  }

  /* ECE552: Store completed. Data block was present with Write permissions in local cache */
  action(s_store_hit, "s", desc="Notify sequencer that store completed.") {
    assert(is_valid(cache_entry));
    DPRINTF(RubySlicc,"%s\n", cache_entry.DataBlk);
    sequencer.writeCallback(address, 
                            GenericMachineType:L1Cache,
                            cache_entry.DataBlk);
  }

  /* ECE552: Store completed. Block was not present in the local cache or had Read_Only permissions. 
     Got data either from the the directory or from the previous owner of that block (i.e., a remote L1 cache) */
  action(sx_store_hit, "sx", desc="External store completed.") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(is_valid(cache_entry));
      DPRINTF(RubySlicc,"%s\n", cache_entry.DataBlk);
      sequencer.writeCallback(address, 
                              getNondirectHitMachType(in_msg.Sender),
                              cache_entry.DataBlk);
    }
  }

  /* ECE552: Copy data block to the cache entry. Note that the is_valid(cache_entry)
     does not check if the block is valid, but rather if it has been allocated (i.e., !NULL pointer)
  */
  action(u_writeDataToCache, "u", desc="Write data to the cache") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(is_valid(cache_entry));
      cache_entry.DataBlk := in_msg.DataBlk;
    }
  }

  /* ECE552: Update the number of pending acknowledgements in the TBE entry.
     When a TBE entry gets allocates pendingAcks is set to 0. The in_msg.AckCount
     when received from the directory is a negative number (i.e., number of sharers)
  */
  action(q_updateAckCount, "q", desc="Update ack count") {
    peek(responseNetwork_in, ResponseMsg) {
      assert(is_valid(tbe));
      tbe.pendingAcks := tbe.pendingAcks - in_msg.AckCount;
      APPEND_TRANSITION_COMMENT(in_msg.AckCount);
      APPEND_TRANSITION_COMMENT(" p: ");
      APPEND_TRANSITION_COMMENT(tbe.pendingAcks);
    }
  }

  /* ECE552: let the sequencer know that the cache replaced a block. */
  action(forward_eviction_to_cpu, "\cc", desc="sends eviction information to the processor") {
    DPRINTF(RubySlicc, "forward_eviction to cpu; send_evictions is %s\n", send_evictions);
    if (send_evictions) {
      DPRINTF(RubySlicc, "Sending invalidation for %s to the CPU\n", address);
      sequencer.evictionCallback(address);
    }
  }

  /* ECE552: allocate a TBE entry. The block is in a transient coherence state. */
  action(v_allocateTBE, "v", desc="Allocate TBE") {
    TBEs.allocate(address);
    set_tbe(TBEs[address]); //Where is the body of set_tbe()?
 			    // It's here (a) mem/slicc/ast/FuncCallExprAST.py
                            //       and (b) mem/slicc/symbols/StateMachine.py
  }

  /* ECE552: Deallocate the TBE entry. The block is now in a stable coherence state and resides in the cache. */
  action(w_deallocateTBE, "w", desc="Deallocate TBE") {
    TBEs.deallocate(address);
    unset_tbe();
  }

  /* ECE552: Copy Data block from cache entry to TBE. Remember a block in transient state
     might still be asked to provide data to forwarded requests */
  action(x_copyDataFromCacheToTBE, "x", desc="Copy data from cache to TBE") {
    assert(is_valid(cache_entry));
    assert(is_valid(tbe));
    tbe.DataBlk := cache_entry.DataBlk;
  }

  /* ECE552: Do nothing. Just stall. */
  action(z_stall, "z", desc="stall") {
    // do nothing
  }

  // =============================================================================
  // TRANSITIONS
  // =============================================================================

  // From I
  transition(I, Load, IS_D) {
    i_allocateL1CacheBlock;
    v_allocateTBE;
    as_issueGetS;
  }
  transition(I, Store, IM_AD) {
    i_allocateL1CacheBlock;
    v_allocateTBE;
    am_issueGetM;
  }

  // From IS_D
  transition(IS_D, {Load, Store, Replacement, Inv}, IS_D) {
    z_stall;
  }
  transition(IS_D, {Data_from_Dir_No_Acks, Data_from_Owner}, S) {
    u_writeDataToCache;
    w_deallocateTBE;
    rx_load_hit;
  }

  // From IM_AD
  transition(IM_AD, {Load, Store, Replacement, Fwd_GetS, Fwd_GetM}, IM_AD) {
    z_stall;
  }
  // If we got all acks already, but not data, we'd still be here.
  // Thus, if we see (...Last), then all acks satisfied.
  transition(IM_AD, {Data_from_Dir_No_Acks, Data_from_Dir_Ack_Cnt_Last, Data_from_Owner}, M) {
    u_writeDataToCache;
    w_deallocateTBE;
    sx_store_hit;
  }
  // As soon as we get data, all we need is acks.
  transition(IM_AD, {Data_from_Dir_Ack_Cnt}, IM_A) {
    u_writeDataToCache;
    q_updateAckCount;
  }
  // No matter how many acks we get while here, we still need data.
  transition(IM_AD, {Inv_Ack}, IM_AD) {
    q_updateAckCount;
  }

  // From IM_A
  transition(IM_A, {Load, Store, Replacement, Fwd_GetS, Fwd_GetM}, IM_A) {
    z_stall;
  }
  transition(IM_A, {Inv_Ack}, IM_A) {
    q_updateAckCount;
  }
  transition(IM_A, {Inv_Ack_all}, M) {
    w_deallocateTBE;
    sx_store_hit;
  }

  // From S
  transition(S, {Load}, S) {
    r_load_hit;
  }
  transition(S, {Store}, SM_AD) {
    v_allocateTBE;
    am_issueGetM;
  }
  transition(S, {Replacement}, SI_A) {
    v_allocateTBE;
    bs_issuePUTS;
  }
  transition(S, {Inv}, I) {
    h_deallocateL1CacheBlock;
    fi_sendInvAck;
  }

  // From SM_AD
  transition(SM_AD, {Load}, SM_AD) {
    r_load_hit;
  }
  transition(SM_AD, {Store, Replacement, Fwd_GetS, Fwd_GetM}, SM_AD) {
    z_stall;
  }
  transition(SM_AD, {Inv}, IM_AD) {
    fi_sendInvAck;
  }
  transition(SM_AD, {Data_from_Dir_No_Acks, Data_from_Dir_Ack_Cnt_Last}, M) {
    u_writeDataToCache;
    w_deallocateTBE;
    sx_store_hit;
  }
  transition(SM_AD, {Data_from_Dir_Ack_Cnt}, SM_A) {
    u_writeDataToCache;
    q_updateAckCount;
  }
  transition(SM_AD, {Inv_Ack}, SM_AD) {
    q_updateAckCount;
  }

  // From SM_A
  transition(SM_A, {Load}, SM_A) {
    r_load_hit;
  }
  transition(SM_A, {Store, Replacement, Fwd_GetS, Fwd_GetM}, SM_A) {
    z_stall;
  }
  transition(SM_A, {Inv_Ack}, SM_A) {
    q_updateAckCount;
  }
  transition(SM_A, {Inv_Ack_all}, M) {
    w_deallocateTBE;
    sx_store_hit;
  }

  // From M
  transition(M, {Load}, M) {
    r_load_hit;
  }
  transition(M, {Store}, M) {
    s_store_hit;
  }
  transition(M, {Replacement}, MI_A) {
    v_allocateTBE;
    x_copyDataFromCacheToTBE;
    de_sendDataFromCacheToDir;
    h_deallocateL1CacheBlock;
    bm_issuePUTM;
  }
  transition(M, {Fwd_GetS}, S) {
    de_sendDataFromCacheToDir;
    e_sendDataFromCacheToRequestor;
  }
  transition(M, {Fwd_GetM}, I) {
    e_sendDataFromCacheToRequestor;
    h_deallocateL1CacheBlock;
  }

  // From MI_A
  transition(MI_A, {Load, Store, Replacement}, MI_A) {
    z_stall;
  }
  transition(MI_A, {Fwd_GetS}, SI_A) {
    dee_sendDataFromTBEToDir;
    ee_sendDataFromTBEToRequestor;
  }
  transition(MI_A, {Fwd_GetM}, II_A) {
    ee_sendDataFromTBEToRequestor;
  }
  transition(MI_A, {WB_Ack}, I) {
    w_deallocateTBE;
  }

  // From SI_A
  transition(SI_A, {Load, Store, Replacement}, SI_A) {
    z_stall;
  }
  transition(SI_A, {Inv}, II_A) {
    fi_sendInvAck;
  }
  transition(SI_A, {WB_Ack}, I) {
    w_deallocate_TBE;
  }
}
